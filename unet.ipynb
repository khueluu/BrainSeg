{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![UNET Architecture](UNET_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3),\n",
    "        nn.ReLU(inplace=True))\n",
    "\n",
    "def up_trans(in_channels, out_channels):\n",
    "    return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "def crop(original_tensor, target_tensor):\n",
    "    target_size = target_tensor.size()[2]\n",
    "    original_size = original_tensor.size()[2]\n",
    "    delta = abs(original_size - target_size)\n",
    "    start = delta // 2\n",
    "    end = original_size - start\n",
    "    return original_tensor[:, :, start:end, start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.down_conv_1 = double_conv(1, 64)\n",
    "        self.down_conv_2 = double_conv(64, 128)\n",
    "        self.down_conv_3 = double_conv(128, 256)\n",
    "        self.down_conv_4 = double_conv(256, 512)\n",
    "        self.down_conv_5 = double_conv(512, 1024)\n",
    "        \n",
    "        self.up_trans_6 = up_trans(1024, 512)\n",
    "        self.up_trans_7 = up_trans(512, 256)\n",
    "        self.up_trans_8 = up_trans(256, 128)\n",
    "        self.up_trans_9 = up_trans(128, 64)\n",
    "        \n",
    "        self.up_conv_6 = double_conv(1024, 512)\n",
    "        self.up_conv_7 = double_conv(512, 256)\n",
    "        self.up_conv_8 = double_conv(256, 128)\n",
    "        self.up_conv_9 = double_conv(128, 64)\n",
    "        \n",
    "        self.out = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, img, verbose=0):\n",
    "        # Contracting path\n",
    "        # Block 1\n",
    "        contracting_1 = self.down_conv_1(img)\n",
    "        if verbose:\n",
    "            print(f'Conntracting Block 1: {contracting_1.shape}')\n",
    "        \n",
    "        # Block 2\n",
    "        contracting_2 = self.max_pool_2x2(contracting_1)  \n",
    "        contracting_2 = self.down_conv_2(contracting_2)\n",
    "        if verbose:\n",
    "            print(f'Conntracting Block 2: {contracting_2.shape}')\n",
    "\n",
    "        # Block 3\n",
    "        contracting_3 = self.max_pool_2x2(contracting_2)\n",
    "        contracting_3 = self.down_conv_3(contracting_3)\n",
    "        if verbose:\n",
    "            print(f'Conntracting Block 3: {contracting_3.shape}')\n",
    "\n",
    "        # Block 4\n",
    "        contracting_4 = self.max_pool_2x2(contracting_3)\n",
    "        contracting_4 = self.down_conv_4(contracting_4)\n",
    "        if verbose:\n",
    "            print(f'Conntracting Block 4: {contracting_4.shape}')\n",
    "\n",
    "        # Block 5\n",
    "        contracting_5 = self.max_pool_2x2(contracting_4)\n",
    "        contracting_5 = self.down_conv_5(contracting_5)\n",
    "        if verbose:\n",
    "            print(f'Conntracting Block 5: {contracting_5.shape}')\n",
    "        \n",
    "        # Expansive path\n",
    "        # Block 6\n",
    "        expansive_6 = self.up_trans_6(contracting_5)\n",
    "        contracting_4_cropped = crop(contracting_4, expansive_6)\n",
    "        concat = torch.cat([contracting_4_cropped, expansive_6], dim=1)\n",
    "        expansive_6 = self.up_conv_6(concat)\n",
    "        if verbose:\n",
    "            print(f'Expansive Block 6: {expansive_6.shape}')\n",
    "        \n",
    "        # Block 7\n",
    "        expansive_7 = self.up_trans_7(expansive_6)\n",
    "        contracting_3_cropped = crop(contracting_3, expansive_7)\n",
    "        concat = torch.cat([contracting_3_cropped, expansive_7], dim=1)\n",
    "        expansive_7 = self.up_conv_7(concat)\n",
    "        if verbose:\n",
    "            print(f'Expansive Block 7: {expansive_7.shape}')\n",
    "        \n",
    "        # Block 8\n",
    "        expansive_8 = self.up_trans_8(expansive_7)\n",
    "        contracting_2_cropped = crop(contracting_2, expansive_8)\n",
    "        concat = torch.cat([contracting_2_cropped, expansive_8], dim=1)\n",
    "        expansive_8 = self.up_conv_8(concat)\n",
    "        if verbose:\n",
    "            print(f'Expansive Block 8: {expansive_8.shape}')\n",
    "        \n",
    "        # Block 9\n",
    "        expansive_9 = self.up_trans_9(expansive_8)\n",
    "        contracting_1_cropped = crop(contracting_1, expansive_9)\n",
    "        concat = torch.cat([contracting_1_cropped, expansive_9], dim=1)\n",
    "        expansive_9 = self.up_conv_9(concat)\n",
    "        output = self.out(expansive_9)\n",
    "        if verbose:\n",
    "            print(f'Expansive Block 9: {expansive_9.shape}')\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([1, 1, 572, 572])\n",
      "Conntracting Block 1: torch.Size([1, 64, 568, 568])\n",
      "Conntracting Block 2: torch.Size([1, 128, 280, 280])\n",
      "Conntracting Block 3: torch.Size([1, 256, 136, 136])\n",
      "Conntracting Block 4: torch.Size([1, 512, 64, 64])\n",
      "Conntracting Block 5: torch.Size([1, 1024, 28, 28])\n",
      "Expansive Block 6: torch.Size([1, 512, 52, 52])\n",
      "Expansive Block 7: torch.Size([1, 256, 100, 100])\n",
      "Expansive Block 8: torch.Size([1, 128, 196, 196])\n",
      "Expansive Block 9: torch.Size([1, 64, 388, 388])\n",
      "Output: torch.Size([1, 1, 388, 388])\n"
     ]
    }
   ],
   "source": [
    "# Test U-Net with 1 random image\n",
    "model = UNet()\n",
    "img = torch.rand(1, 1, 572, 572) # batch_size, channel, height, width\n",
    "print(f'Input: {img.shape}')\n",
    "output = model(img, verbose=1)\n",
    "print(f'Output: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "        self.masks = os.listdir(mask_dir)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Load\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[index].replace('.tif', '_mask.tif'))\n",
    "        assert os.path.exists(img_path)\n",
    "        assert os.path.exists(mask_path)\n",
    "        \n",
    "        # Convert to grayscale\n",
    "        image = np.array(Image.open(img_path).resize((572,572)).convert('L'), dtype=np.float32)\n",
    "        mask = np.array(Image.open(mask_path).resize((388,388)), dtype=np.float32)\n",
    "\n",
    "        # Resize\n",
    "        resized_img = np.zeros((1,572,572))\n",
    "        resized_img[0,:,:] = image\n",
    "        resized_mask = np.zeros((1,388,388))\n",
    "        resized_mask[0,:,:] = mask\n",
    "        \n",
    "        # Convert to binary\n",
    "        max_ = 255.0\n",
    "        resized_mask = resized_mask/max_\n",
    "        \n",
    "        # Convert to tensor\n",
    "        tensor_img = torch.from_numpy(resized_img)\n",
    "        tensor_mask = torch.from_numpy(resized_mask)\n",
    "        \n",
    "        # Apply transformation\n",
    "        if self.transform is not None:\n",
    "            tensor_img = self.transform(tensor_img)\n",
    "            \n",
    "        return tensor_img, tensor_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "TRAIN_IMG_DIR = 'data/train_images/'\n",
    "TRAIN_MASK_DIR = 'data/train_masks/'\n",
    "VAL_IMG_DIR = 'data/val_images/'\n",
    "VAL_MASK_DIR = 'data/val_masks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MRIDataset(TRAIN_IMG_DIR, TRAIN_MASK_DIR)\n",
    "val_set = MRIDataset(VAL_IMG_DIR, VAL_MASK_DIR)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 572, 572]), torch.Size([1, 388, 388]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of image\n",
    "img, mask = train_set[0]\n",
    "img.shape, mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_dice(prediction, ground_truth):\n",
    "    n_images = len(prediction)\n",
    "    loss = 2*torch.mul(prediction, ground_truth).sum()\n",
    "    loss /= (prediction.sum() + ground_truth.sum())\n",
    "    return loss\n",
    "    # return torch.clamp(loss, min=1e-7, max=1-1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, tensor(0.4992))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.rand(1,1,338,338)\n",
    "truth = torch.rand(1,1,338,338)\n",
    "test_loss = soft_dice(pred, truth)\n",
    "test_loss.dtype, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, validation\n",
    "# Next time: show train result\n",
    "# Ask in telegram chat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Epoch 0 ==========\n",
      "---------- Batch 1 ----------\n",
      "---------- Batch 2 ----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5cc4c76f8f46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-723251d051ac>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img, verbose)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# Block 9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mexpansive_9\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_trans_9\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpansive_8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mcontracting_1_cropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontracting_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpansive_9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mconcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontracting_1_cropped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpansive_9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    925\u001b[0m             input, output_size, self.stride, self.padding, self.kernel_size, self.dilation)\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m         return F.conv_transpose2d(\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             output_padding, self.groups, self.dilation)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "train_loss_history = []\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'========== Epoch {epoch} ==========')\n",
    "    counter = 1\n",
    "    for X, y in train_loader:\n",
    "        print(f'---------- Batch {counter} ----------')\n",
    "        counter += 1\n",
    "        \n",
    "        # Get prediction from the model\n",
    "        X = X.float()\n",
    "        y = y.float()\n",
    "        pred = model(X)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = soft_dice(pred, y)\n",
    "        \n",
    "        # Calculate gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimize\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        # Save train loss history\n",
    "        train_loss_history.append(loss.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Trainer(model, train_loader, val_loader,  optimizer, epochs, sheduler = None):\n",
    "    train_loss_history, train_accuracy, train_dice= [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train(True)\n",
    "        for (X, y) in train_loader:\n",
    "            # loss, accuracy= compute_loss(X, y)\n",
    "            # dice = dice_coef(y,X)\n",
    "            loss = compute_loss(X, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            train_loss_history.append(loss.data.cpu().numpy())\n",
    "            # train_accuracy.append(accuracy)\n",
    "            # train_dice.append(dice)\n",
    "        \n",
    "        clear_output()\n",
    "        if sheduler is not None:\n",
    "            sheduler.step(train_loss_history[-1])\n",
    "        print(\"Last loss:\\t{}\\nEpoch number:\\t{}\\nCurrent Learning rate:{}\".format(train_loss_history[-1], epoch, optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "        plt.plot(train_loss_history)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "model = UNet()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "shedule = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', patience=20)\n",
    "num_epochs = 1\n",
    "batch_size = 20\n",
    "\n",
    "train_batch = train_set[:batch_size]\n",
    "test_batch = test_set[:batch_size]\n",
    "\n",
    "try:\n",
    "    Trainer(model, train_batch, test_batch, epochs=num_epochs, optimizer=opt, sheduler=shedule)\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
